{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160104b3",
   "metadata": {},
   "source": [
    "- The code is written in Python and it is using the PySpark library to perform data manipulation and analysis.\n",
    "- `mpg` is a DataFrame or a table-like data structure in PySpark that contains information about car mileage.\n",
    "- `mpg.select(...)` is a method that allows you to select specific columns from the DataFrame. In this case, it is selecting the \"hwy\" column.\n",
    "- `mpg.hwy.alias(\"highway_mileage\")` is an operation on the \"hwy\" column. It renames the column to \"highway_mileage\" using the `alias()` method. This will create a new column in the resulting DataFrame.\n",
    "- `.show(5)` is a method that displays the resulting DataFrame. The argument \"5\" specifies that it should show the first 5 rows of the DataFrame.\n",
    "- Overall, this code selects the \"hwy\" column from the \"mpg\" DataFrame and renames it as \"highway_mileage\". It then displays the first 5 rows of the resulting DataFrame, showing the values in the \"highway_mileage\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82973da4",
   "metadata": {},
   "source": [
    "### Create a spark data frame that contains your favorite programming languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff3307",
   "metadata": {},
   "source": [
    "- This code creates a SparkSession, which is the entry point to interact with Spark.\n",
    "- The data is defined as a list of tuples, where each tuple contains a single value representing a programming language.\n",
    "- spark.createDataFrame(data, [\"language\"]) creates a DataFrame named df with a single column named \"language\".\n",
    "- df.printSchema() displays the schema of the DataFrame, which shows the column name and its data type.\n",
    "- The shape of the DataFrame is calculated using df.count() to get the number of rows and len(df.columns) to get the number of columns.\n",
    "- The first 5 records in the DataFrame are shown using df.show(5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the data\n",
    "data = [(\"Python\",), (\"Java\",), (\"JavaScript\",), (\"C++\",), (\"Ruby\",)]\n",
    "\n",
    "# Create a DataFrame with a single column named \"language\"\n",
    "df = spark.createDataFrame(data, [\"language\"])\n",
    "\n",
    "# View the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Output the shape of the DataFrame\n",
    "shape = (df.count(), len(df.columns))\n",
    "print(\"Shape of the DataFrame:\", shape)\n",
    "\n",
    "# Show the first 5 records in the DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f9522",
   "metadata": {},
   "source": [
    "### Load the mpg dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mpg dataset as a DataFrame\n",
    "mpg = spark.read.csv(\"mpg.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a new column named \"output\" with the desired message\n",
    "mpg = mpg.withColumn(\"output\", F.concat(F.lit(\"The \"), mpg.year, F.lit(\" \"), mpg.manufacturer, F.lit(\" \"), mpg.model, F.lit(\" has a \"), mpg.cyl, F.lit(\" cylinder engine.\")))\n",
    "\n",
    "# Transform the \"trans\" column to only contain \"manual\" or \"auto\"\n",
    "mpg = mpg.withColumn(\"trans\", F.when(F.col(\"trans\").like(\"%manual%\"), \"manual\").otherwise(\"auto\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be00e0",
   "metadata": {},
   "source": [
    "The code assumes that the \"mpg.csv\" file is present in the current directory.\n",
    "spark.read.csv(\"mpg.csv\", header=True, inferSchema=True) loads the dataset as a DataFrame named \"mpg\". The header=True argument indicates that the first row of the CSV file contains column names, and inferSchema=True infers the data types of the columns.\n",
    "mpg.withColumn(\"output\", ...) adds a new column named \"output\" to the \"mpg\" DataFrame using the withColumn() method and the concat() function from the pyspark.sql.functions module. It concatenates the desired message using the existing columns.\n",
    "mpg.withColumn(\"trans\", ...) transforms the \"trans\" column to contain only \"manual\" or \"auto\" values using the withColumn() method and the when() and otherwise() functions from the pyspark.sql.functions module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db6d9a",
   "metadata": {},
   "source": [
    "### Load the tips dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tips dataset as a DataFrame\n",
    "tips = spark.read.csv(\"tips.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Calculate the percentage of observations that are smokers\n",
    "smokers_percentage = (tips.filter(F.col(\"smoker\") == \"Yes\").count() / tips.count()) * 100\n",
    "print(\"Percentage of observations that are smokers:\", smokers_percentage)\n",
    "\n",
    "# Create a column named \"tip_percentage\"\n",
    "tips = tips.withColumn(\"tip_percentage\", (F.col(\"tip\") / F.col(\"total_bill\")) * 100)\n",
    "\n",
    "# Calculate the average tip percentage for each combination of sex and smoker\n",
    "average_tip_percentage = tips.groupby(\"sex\", \"smoker\").agg(F.avg(\"tip_percentage\").alias(\"avg_tip_percentage\"))\n",
    "average_tip_percentage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befbd6e",
   "metadata": {},
   "source": [
    "The code assumes that the \"tips.csv\" file is present in the current directory.\n",
    "spark.read.csv(\"tips.csv\", header=True, inferSchema=True) loads the dataset as a DataFrame named \"tips\".\n",
    "The percentage of observations that are smokers is calculated by filtering the DataFrame for rows where the \"smoker\" column is \"Yes\", counting the number of rows, dividing it by the total number of rows, and multiplying by 100.\n",
    "tips.withColumn(\"tip_percentage\", ...) adds a new column named \"tip_percentage\" to the \"tips\" DataFrame. The column is calculated by dividing the \"tip\" column by the \"total_bill\" column and multiplying by 100.\n",
    "The average tip percentage for each combination of \"sex\" and \"smoker\" is calculated by grouping the DataFrame by those columns and calculating the average of the \"tip_percentage\" column using the groupby(), agg(), and avg() functions from the pyspark.sql.functions module. The resulting DataFrame is displayed using show()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e77114",
   "metadata": {},
   "source": [
    "### Use the seattle weather dataset referenced in the lesson to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seattle weather dataset as a DataFrame\n",
    "weather = spark.read.csv(\"seattle_weather.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert the temperatures to Fahrenheit\n",
    "weather = weather.withColumn(\"temp_max_fahrenheit\", (weather[\"temp_max\"] * 9/5) + 32)\n",
    "weather = weather.withColumn(\"temp_min_fahrenheit\", (weather[\"temp_min\"] * 9/5) + 32)\n",
    "\n",
    "# Which month has the most rain, on average?\n",
    "most_rain_month = weather.groupby(\"month\").agg(F.avg(\"precipitation\").alias(\"avg_precipitation\")).orderBy(F.desc(\"avg_precipitation\")).first()\n",
    "print(\"Month with the most rain, on average:\", most_rain_month[\"month\"])\n",
    "\n",
    "# Which year was the windiest?\n",
    "windiest_year = weather.groupby(\"year\").agg(F.sum(\"wind\").alias(\"total_wind\")).orderBy(F.desc(\"total_wind\")).first()\n",
    "print(\"Year with the highest wind, on average:\", windiest_year[\"year\"])\n",
    "\n",
    "# What is the most frequent type of weather in January?\n",
    "january_weather = weather.filter(F.col(\"month\") == 1)\n",
    "most_frequent_weather = january_weather.groupby(\"weather\").count().orderBy(F.desc(\"count\")).first()\n",
    "print(\"Most frequent type of weather in January:\", most_frequent_weather[\"weather\"])\n",
    "\n",
    "# What is the average high and low temperature on sunny days in July in 2013 and 2014?\n",
    "july_sunny_days = weather.filter((F.col(\"month\") == 7) & (F.col(\"weather\") == \"sun\"))\n",
    "july_sunny_days_2013_2014 = july_sunny_days.filter((F.col(\"year\") == 2013) | (F.col(\"year\") == 2014))\n",
    "average_high_low_temp = july_sunny_days_2013_2014.agg(F.avg(\"temp_max_fahrenheit\").alias(\"avg_temp_max\"), F.avg(\"temp_min_fahrenheit\").alias(\"avg_temp_min\")).first()\n",
    "print(\"Average high temperature on sunny days in July in 2013 and 2014:\", average_high_low_temp[\"avg_temp_max\"])\n",
    "print(\"Average low temperature on sunny days in July in 2013 and 2014:\", average_high_low_temp[\"avg_temp_min\"])\n",
    "\n",
    "# What percentage of days were rainy in Q3 of 2015?\n",
    "q3_2015_rainy_days = weather.filter((F.col(\"year\") == 2015) & (F.col(\"quarter\") == 3) & (F.col(\"weather\") == \"rain\"))\n",
    "rainy_days_percentage = (q3_2015_rainy_days.count() / weather.filter((F.col(\"year\") == 2015) & (F.col(\"quarter\") == 3)).count()) * 100\n",
    "print(\"Percentage of rainy days in Q3 of 2015:\", rainy_days_percentage)\n",
    "\n",
    "# For each year, find what percentage of days it rained (had non-zero precipitation)\n",
    "rainy_days_percentage_by_year = weather.groupby(\"year\").agg((F.sum(F.when(F.col(\"precipitation\") > 0, 1).otherwise(0)) / F.count(\"*\")) * 100).withColumnRenamed(\"((sum(CASE WHEN (precipitation > 0) THEN 1 ELSE 0 END) / count(1)) * 100)\", \"rainy_days_percentage\")\n",
    "rainy_days_percentage_by_year.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c675df6",
   "metadata": {},
   "source": [
    "The code assumes that the \"seattle_weather.csv\" file is present in the current directory.\n",
    "spark.read.csv(\"seattle_weather.csv\", header=True, inferSchema=True) loads the dataset as a DataFrame named \"weather\".\n",
    "The temperatures are converted to Fahrenheit by creating new columns using the appropriate conversion formulas.\n",
    "The month with the most average rain is determined by grouping the DataFrame by \"month\" and calculating the average precipitation using the groupby(), agg(), and avg() functions. The resulting DataFrame is ordered in descending order by average precipitation, and the first row is selected to get the month with the most rain.\n",
    "The windiest year is determined by grouping the DataFrame by \"year\" and calculating the sum of the \"wind\" column. The resulting DataFrame is ordered in descending order by total wind, and the first row is selected to get the windiest year.\n",
    "The most frequent type of weather in January is determined by filtering the DataFrame for rows where the \"month\" column is 1, grouping by \"weather\" and counting the occurrences. The resulting DataFrame is ordered in descending order by count, and the first row is selected to get the most frequent weather type.\n",
    "The average high and low temperature on sunny days in July in 2013 and 2014 is determined by filtering the DataFrame for rows where the \"month\" column is 7 and the \"weather\" column is \"sun\", and then filtering for the years 2013 and 2014. The average temperatures are calculated using the avg() function and are displayed.\n",
    "The percentage of rainy days in Q3 of 2015 is determined by filtering the DataFrame for rows where the \"year\" column is 2015, the \"quarter\" column is 3, and the \"weather\" column is \"rain\". The count of rainy days is divided by the count of all days in Q3 of 2015, and the result is multiplied by 100 to get the percentage.\n",
    "For each year, the percentage of days with non-zero precipitation is calculated by grouping the DataFrame by \"year\", using the sum(), when(), otherwise(), and count() functions to count the number of days with non-zero precipitation and the total number of days, and calculating the percentage. The resulting DataFrame is displayed using show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac36d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
